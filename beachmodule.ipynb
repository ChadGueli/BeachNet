{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from kaggle_datasets import KaggleDatasets\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","from tensorflow import keras\n","from tensorflow.keras.layers import DenseFeatures, RepeatVector, Dense\n","from tensorflow.keras.layers import ZeroPadding2D, GlobalAveragePooling2D\n","from tensorflow.keras import backend as K\n","from tensorflow.python.feature_column import feature_column_v2 as fc\n","from tensorflow.python.feature_column import sequence_feature_column as seq_fc\n","import pandas as pd\n","from random import randint"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["BATCH_SIZE = 32 #1024\n","SEQ_LEN = 1941\n","SET_SIZE = 42840\n","TRAIN_SIZE = 16 * SET_SIZE // 25\n","VAL_SIZE = 4 * SET_SIZE // 25\n","GCS_PATH = KaggleDatasets().get_gcs_path(\"complete-m5-uncertainty-validation-records\")"]},{"cell_type":"markdown","metadata":{},"source":["### Prep for Data Import"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cal_dict = pd.read_csv(\n","    \"../input/m5-forecasting-uncertainty/calendar.csv\"\n",").rename(\n","    columns={\"event_name_1\": \"event\",\n","             \"wday\": \"day\"}\n",").fillna(\n","    {\"event\": \"none\"}\n",").astype(\n","    {\"event\": pd.StringDtype()}\n",").assign(\n","    day=lambda x: x.day - 1,\n","    month=lambda x: x.month - 1,\n","    year=lambda x: x.year - 2011\n",").loc[:, \"day\":\"event\"].to_dict(orient=\"list\")\n","del cal_dict[\"d\"]\n","\n","def to_apropros_tensor(array):\n","    tensor = tf.constant(array)\n","    shaped_tensor = tensor[tf.newaxis, ...]\n","    repeat_tensor = tf.repeat(shaped_tensor[:, :SEQ_LEN - 28],\n","                             [BATCH_SIZE], axis=0)\n","    \n","    # this conditional is necessary because `tf.sparse.from_dense`\n","    # only supports numeric types\n","    if repeat_tensor.dtype == tf.string:\n","        #bool_tensor = repeat_tensor != \"none\"\n","        return tf.sparse.SparseTensor(\n","            tf.where(tf.ones(repeat_tensor.shape)),\n","            tf.reshape(repeat_tensor, [-1]),\n","            repeat_tensor.shape)\n","    \n","    else:\n","        return tf.sparse.from_dense(repeat_tensor)\n","\n","cal_dict = {col: to_apropros_tensor(array)\n","                for col, array in cal_dict.items()}"]},{"cell_type":"markdown","metadata":{},"source":["The following produces the feature descriptions that will be used for parsing the Example protos, and implementing the feature column transformations."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Defines the context feature column specifications.\n","keys_and_buckets = {\n","    \"dept_id\": 3,\n","    \"cat_id\": 3,\n","    \"store_id\": 4,\n","    \"state_id\": 3\n","}\n","\n","columns = [tf.feature_column.categorical_column_with_identity(\n","    key, num_buckets\n",") for key, num_buckets in keys_and_buckets.items()]\n","\n","columns = [tf.feature_column.indicator_column(col)\n","                for col in columns] # FUCKED WITH ALL OF THIS\n","\n","item = fc.categorical_column_with_hash_bucket(\"item_id\", 1000)\n","cntx_ftr.append(fc.embedding_column(item, 5))\n","                \n","# Defines the sequence feature column specifications.\n","seq_ftr = [seq_fc.sequence_categorical_column_with_identity(\n","    key, num_buckets\n",") for key, num_buckets in zip([\"day\", \"month\", \"year\"], [7, 12, 6])]\n","\n","seq_ftr = [fc.indicator_column(col) for col in seq_ftr]\n","\n","event = seq_fc.sequence_categorical_column_with_hash_bucket(\"event\", 30)\n","seq_ftr.extend([    \n","    fc.embedding_column(event, 2),\n","    seq_fc.sequence_numeric_column(\"units\")\n","])"]},{"cell_type":"markdown","metadata":{},"source":["In the next block, the `cntx_ftr_prs` and `seq_ftr_prs` dicts are for parsing the context and sequence features respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cntx_ftr_prs = tf.feature_column.make_parse_example_spec(cntx_ftr)\n","cntx_ftr_prs[\"weights\"] = tf.io.FixedLenFeature([], tf.float32)\n","cntx_ftr_prs[\"id\"] = tf.io.FixedLenFeature([], tf.string)\n","\n","seq_ftr_prs = tf.feature_column.make_parse_example_spec(seq_ftr[-1:])"]},{"cell_type":"markdown","metadata":{},"source":["### Custom layers"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# CHECK\n","class CausalConv2D(keras.layers.Conv2D):\n","    \"\"\"This is an extremely ad hoc layer designed to perform causal 2d\n","    convolutions during the phase when the width of the layer is expanded.\n","    As such, there is limited accessibility; for instance, use of bias is\n","    necessary.\"\"\"\n","    \n","    def call(self, inputs):\n","        outputs = tf.nn.conv2d(\n","            inputs, filters=self.kernel, strides=self.strides,\n","            padding=[[0, 0], [self.kernel_size[0] // 2 + 1] * 2,\n","                     [self.kernel_size[1] - 1, 0], [0, 0]],\n","            dilations=self.dilation_rate)\n","        \n","        outputs = tf.nn.bias_add(outputs, self.bias)\n","        return self.activation(outputs)\n","    \n","    def compute_output_shape(self, input_shape):\n","        output_shape = tf.TensorShape(input_shape).as_list()\n","        output_shape[1] += 2\n","        output_shape[-1] = self.filters\n","        return tf.TensorShape(output_shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# CHECK\n","class CausalSeperableConv2D(keras.layers.SeparableConv2D):\n","    \"\"\"This is an extremely ad hoc layer. It was construted to perform\n","    causal, seperable 2d convolutions. As such, there is limited\n","    customizability; for instance, use of bias is necessary.\"\"\"\n","    \n","    def call(self, inputs):\n","        outputs = K.spatial_2d_padding(\n","            inputs, padding=((1, 1), (2 * self.dilation_rate[1], 0)))\n","        outputs = tf.nn.depthwise_conv2d(\n","            outputs, self.depthwise_kernel, strides=(1, *self.strides, 1),\n","            padding=\"VALID\",\n","            dilations=self.dilation_rate)\n","        outputs = tf.nn.conv2d(\n","            inputs, self.pointwise_kernel, strides=1, padding=\"VALID\")\n","        outputs = tf.nn.bias_add(outputs, self.bias)\n","        return self.activation(outputs)\n","    \n","    def compute_output_shape(self, input_shape):\n","        output_shape = tf.TensorShape(input_shape).as_list()\n","        output_shape[-1] = self.filters\n","        return tf.TensorShape(output_shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Custom Blocks"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# CHECK\n","class SEBlock(keras.layers.Layer):\n","    \"\"\"A simple squeeze and excitation block as originally described\n","    in...\"\"\"\n","    \n","    def __init__(self, n_filters, context=True, squeeze_factor=16, **kwargs):\n","        super().__init__(**kwargs)      \n","        self.context = context\n","        self.global_avg_pool = GlobalAveragePooling2D()\n","        self.dense_1 = Dense(n_filters // squeeze_factor, activation=\"swish\",\n","                             kernel_initializer=\"he_normal\")\n","        \n","        self.dense_2 = Dense(n_filters, activation=\"sigmoid\",\n","                             kernel_initializer=\"glorot_normal\")\n","    \n","    def call(self, inputs):\n","        if self.context:\n","            inputs, context = inputs\n","            \n","        outputs = self.global_avg_pool(inputs)\n","        if self.context:\n","            context = keras.layers.add([outputs, context])\n","            outputs = self.dense_1(context)\n","            return self.dense_2(outputs), context\n","        else:\n","            outputs = self.dense_1(outputs)\n","            return self.dense_2(outputs)\n","    \n","    def get_config(self):\n","        base_config = super().get_config()\n","        return {\n","            **base_config,\n","            \"n_filters\": self.dense_2.units,\n","            \"context\": self.context,\n","            \"squeeze_factor\": self.dense_2.units // self.dense_1.units # yes, this may be larger than the passed\n","                                                                       # squeeze_factor, and no it does not\n","                                                                       # matter because we will have the same\n","                                                                       # implemented squeeze_factor\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# CHECK\n","# should add ability to stop context flow\n","class BeachModule(keras.layers.Layer):\n","    \"\"\"Implements the `CausalSeperableConv2D` class defined earlier\n","    which applies a seperable convolution, with SAME padding in the\n","    height (quantiles) dimension and causal padding in the width\n","    (time) dimension. It then applies a squeeze and excitation\n","    layer, due to the information loss induced by global average\n","    pooling, and the squeeze layer, this block should not break\n","    causality during training.\n","    \n","    MAKE A NOTE ABOUT CONTEXT.\"\"\"\n","    \n","    def __init__(self, n_filters, dilation_rate,\n","                 squeeze_factor=16,\n","                 context=True, **kwargs):\n","        super().__init__(**kwargs)\n","        self.causal_sep_atrous_conv = CausalSeperableConv2D(\n","            n_filters, kernel_size=3,\n","            dilation_rate=(1, dilation_rate),\n","            activation= \"swish\",\n","            depthwise_initializer=\"he_normal\",\n","            pointwise_initializer=\"he_normal\")\n","        \n","        self.context = context\n","        self.se_block = SEBlock(n_filters, squeeze_factor=squeeze_factor,\n","                                context=context)\n","        if self.context:\n","            self.dense = Dense(n_filters, activation=\"swish\",\n","                               kernel_initializer=\"he_normal\")\n","    \n","    def call(self, inputs):\n","        inputs, context = inputs\n","        outputs = self.causal_sep_atrous_conv(inputs)\n","        if self.context:\n","            context = self.dense(context)\n","            se, context = self.se_block([outputs, context])\n","        else:\n","            se = self.se_block(outputs)\n","            \n","        outputs = keras.layers.multiply([outputs, se])\n","        return [outputs + inputs, context, outputs]\n","    \n","    #def compute_output_shape(self, batch_input_shape):\n","    #    return [*batch_input_shape, batch_input_shape[0]]\n","    \n","    def get_config(self):\n","        base_config = super().get_config()\n","        return {\n","            **base_config,\n","            \"n_filters\": self.causal_sep_atrous_conv.filters,\n","            \"dilation_rate\": self.causal_sep_atrous_conv.dilation_rate[1],\n","            \"context\": self.context,\n","            \"squeeze_factor\": self.se_block.get_config()[\"squeeze_factor\"]\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# CHECK\n","class XWideWave(keras.layers.Layer):\n","    \n","    def __init__(self, n_layers, n_filters, **kwargs):\n","        super().__init__(**kwargs)\n","        self.causal_conv = CausalConv2D(\n","            n_filters, kernel_size=3,\n","            activation=\"swish\",\n","            kernel_initializer=\"he_normal\")\n","        \n","        self.layers = [BeachModule(n_filters, 3 ** i)\n","                           for i in range(n_layers)]\n","    \n","    def call(self, inputs):\n","        inputs, context = inputs\n","        outputs = self.causal_conv(inputs)\n","        \n","        skips = tf.zeros([1, 1, 1, self.causal_conv.filters])\n","        for layer in self.layers:\n","            outputs, context, skip = layer([outputs, context])\n","            skips += skip\n","        \n","        return [outputs, context, skips]\n","    \n","    def get_config(self):\n","        base_config = super().get_config()\n","        return {\n","            **base_config,\n","            \"n_filters\": self.causal_conv.filters,\n","            \"n_layers\": len(self.layers)\n","        }"]},{"cell_type":"markdown","metadata":{},"source":["### Other Training Stuff"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Check\n","class ScaledPinballLoss(keras.losses.Loss):\n","    \n","    def __init__(self, memory=1, **kwargs):\n","        super().__init__(**kwargs)\n","        self.memory = -memory\n","        self.taus = tf.constant(\n","            [0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995],\n","            shape=[9, 1, 1])\n","    \n","    def call(self, y_true, q_pred):\n","        abs_diff = tf.squeeze(tf.math.abs(y_true[:, 1:-27] - y_true[:, 0:-28]))\n","        if self.memory == -1:\n","            mad = tf.math.reduce_mean(abs_diff, axis=-1, keepdims=True)\n","        else:\n","            mad = tf.math.cumsum(abs_diff, axis=-1)\n","            mad /= tf.range(1, SEQ_LEN - 27, dtype=tf.float32)\n","            mad = tf.where(mad == 0, 1., mad)\n","            mad = mad[:, self.memory:]\n","                   \n","        # the following reshapes and copies `y_true` into the necessary\n","        # form for the evaluation of `q_pred`\n","        y_true = y_true[:, self.memory - 27:]\n","        q_pred = q_pred[..., self.memory:, :]\n","        \n","        if self.memory == -1:\n","            y_true = tf.transpose(y_true, perm=[0, 2, 1])\n","        else:\n","            stacks = [y_true[:, i: i - 27] for i in range(0, 27)]\n","            stacks.append(y_true[:, 27:])\n","            y_true = tf.concat(stacks, axis=-1)\n","                                          \n","        out = tf.expand_dims(y_true, axis=1) - q_pred\n","        out *= tf.where(out < 0, self.taus - 1., self.taus)\n","        return tf.math.reduce_mean(out, [1, -1]) / mad \n","    \n","    def __call__(self, y_true, y_pred, sample_weight=None):\n","        if sample_weight is not None and self.memory == -1:\n","            sample_weight -= 1\n","        \n","        return super().__call__(y_true, y_pred, sample_weight=sample_weight)"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# CHECK\n","def parse(batch):\n","    cntx_dict, seq_dict, _ = tf.io.parse_sequence_example(\n","        batch,\n","        context_features=cntx_ftr_prs,\n","        sequence_features=seq_ftr_prs)\n","    \n","    id_ = cntx_dict.pop(\"id\")\n","    weights = cntx_dict.pop(\"weights\") + 1 # allows us to glean information from\n","                                           # sequences that would otherwise be\n","                                           # so weightless as to have no effect\n","    units = seq_dict[\"units\"]\n","    y_true = tf.sparse.to_dense(units)\n","    units = tf.sparse.slice(units, [0, 0, 0], [BATCH_SIZE, SEQ_LEN - 28, 1])\n","    seq_dict[\"units\"] = units / tf.sparse.reduce_max(units, axis=[1, 2],\n","                                                     keepdims=True)\n","    \n","    seq_dict.update(cal_dict)\n","    return (cntx_dict, seq_dict), y_true, weights\n","    \n","\n","def repeat_batch_and_parse(dataset, batch_size=BATCH_SIZE):\n","    return dataset.repeat(\n","    ).batch(batch_size, drop_remainder=True\n","    ).map(\n","        parse,\n","        num_parallel_calls=tf.data.experimental.AUTOTUNE\n","    ).prefetch(tf.data.experimental.AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lr_scheduler = tfa.optimizers.TriangularCyclicalLearningRate(\n","    initial_learning_rate=0.001,\n","    maximal_learning_rate=0.002,\n","    step_size=4)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
